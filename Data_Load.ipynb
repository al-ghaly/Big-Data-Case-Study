{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ad122e-de11-44c7-b4cd-6d5e69255151",
   "metadata": {},
   "source": [
    "Import needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29af3c8b-f412-4f6f-8b94-8a4bee2ed5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ebb6cc2-43b6-480b-8caa-461bd2b7f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import subprocess\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "56055715-5929-4092-ac18-84ab673341f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"/data/Spark_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb98db4-d292-461a-9f69-89a19d313865",
   "metadata": {},
   "source": [
    "#### Create the directories in HDFS (If not exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "45f03d31-2e72-443e-9605-bea64f090bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /Spark_Project/data/Q_company/branches created successfully in HDFS.\n",
      "Directory /Spark_Project/data/Q_company/sales_agents created successfully in HDFS.\n",
      "Directory /Spark_Project/data/Q_company/sales_transactions created successfully in HDFS.\n"
     ]
    }
   ],
   "source": [
    "def create_hdfs_directory(*paths):\n",
    "    for path in paths:\n",
    "        try:\n",
    "            subprocess.run([\"hdfs\", \"dfs\", \"-mkdir\", \"-p\", path], check=True)\n",
    "            print(f\"Directory {path} created successfully in HDFS.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to create directory {path} in HDFS. Error: {e}\")\n",
    "\n",
    "create_hdfs_directory(\"/Spark_Project/data/Q_company/branches\", \n",
    "                     \"/Spark_Project/data/Q_company/sales_agents\",\n",
    "                     \"/Spark_Project/data/Q_company/sales_transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bb071-12a0-4655-bc1f-60dc9545d747",
   "metadata": {},
   "source": [
    "Create list of unique files to be uploaded to the Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8e7dfdf5-3838-4841-9e14-1c742f9718e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_checksum(file_path, chunk_size = 4096):\n",
    "    # Read the file chunk by chunk to handle large files efficiently\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        hasher = hashlib.md5()\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            hasher.update(chunk)\n",
    "\n",
    "    # Return the hexadecimal representation of the hash\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e6349f32-fa7a-4bc5-8fcc-263ba0be72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    file_hashes = {}\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if os.path.basename(root) == \"archive\":\n",
    "            continue\n",
    "        elif os.path.basename(os.path.dirname(root)) == \"archive\":\n",
    "            continue\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Check the uniqueness of the file using its check sum\n",
    "            checksum = caculate_checksum(file_path)\n",
    "            if checksum in file_hashes:\n",
    "                print(f\"{file} is duplicated!\\nIt is the same as the file: {file_hashes[checksum]}\")\n",
    "                print(\"File Ignored\")\n",
    "                print(\"--\" * 20)\n",
    "                continue\n",
    "            file_hashes[checksum] = file_path\n",
    "    return list(file_hashes.values())\n",
    "files = get_files(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac4198-200f-440e-baff-859464a8fcb6",
   "metadata": {},
   "source": [
    "Add Quality Columns to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "36d4e7de-733b-4766-918f-c12ca1cf04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_path(local_source):\n",
    "    file_name = os.path.basename(local_source)\n",
    "    parent_directory = os.path.basename(os.path.dirname(local_source))\n",
    "    hdfs_file_name = parent_directory + '_' + file_name\n",
    "    \n",
    "    if 'branch' in file_name.lower():\n",
    "        hdfs_directory = \"/Spark_Project/data/Q_company/branches\"\n",
    "    elif 'agent' in file_name.lower():\n",
    "        hdfs_directory = \"/Spark_Project/data/Q_company/sales_agents\"\n",
    "    elif'transaction'in file_name.lower():\n",
    "        hdfs_directory = \"/Spark_Project/data/Q_company/sales_transactions\"\n",
    "    else:\n",
    "        print(\"Unknown File!!\")\n",
    "    return os.path.join(hdfs_directory, hdfs_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc81a0ab-a7d3-489e-aab8-90af64149de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(local_source):\n",
    "    hdfs_destination = get_hdfs_path(local_source)\n",
    "    if hdfs_destination is None:\n",
    "        return False\n",
    "    try:\n",
    "        subprocess.run([\"hdfs\", \"dfs\", \"-put\", \"-f\", local_source, hdfs_destination], check=True)\n",
    "        print(f\"Successfully copied {local_source} to {hdfs_destination} in HDFS.\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to copy {local_source} to {hdfs_destination} in HDFS. Error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "af1f2e4b-11c7-489c-a942-5d784699bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def archive_file(path):\n",
    "    # Create the Archive folder if it does not exist\n",
    "    folder_path =  os.path.join(local_path, 'archive')\n",
    "\n",
    "    # Create the archive directory\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "\n",
    "    # Move the file to the Archive folder\n",
    "    parent_directory = os.path.basename(os.path.dirname(path))\n",
    "    archive_group = os.path.join(folder_path, parent_directory)\n",
    "    # Create folder for this files group in the archive directory\n",
    "    if not os.path.exists(archive_group):\n",
    "        os.makedirs(archive_group)\n",
    "    try:\n",
    "        subprocess.run([\"mv\", path, archive_group], check=True)\n",
    "        print(f\"Successfully moved {path} to Archive.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to move {path} to Archive. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e6a6b056-f697-408c-b4cf-e847e89393a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(files):\n",
    "    for path in files:\n",
    "        df = pd.read_csv(path)\n",
    "        current_timestamp = datetime.datetime.now()\n",
    "        df['load_time'] = str(current_timestamp)\n",
    "        df['source'] = 'Local File System'\n",
    "        df['source_path'] = path\n",
    "        df.to_csv(path, index=False)\n",
    "        upload_status = upload_file(path)\n",
    "        if upload_status:\n",
    "            archive_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669666af-5bd6-4c8e-a0ad-c1263a2542a6",
   "metadata": {},
   "source": [
    "Move the data to Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7a482963-0999-4307-a829-3a4cd192529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied /data/Spark_project/group1/branches_SS_raw_1.csv to /Spark_Project/data/Q_company/branches/group1_branches_SS_raw_1.csv in HDFS.\n",
      "Folder '/data/Spark_project/archive' already exists.\n",
      "Successfully moved /data/Spark_project/group1/branches_SS_raw_1.csv to Archive.\n",
      "Successfully copied /data/Spark_project/group1/sales_agents_SS_raw_1.csv to /Spark_Project/data/Q_company/sales_agents/group1_sales_agents_SS_raw_1.csv in HDFS.\n",
      "Folder '/data/Spark_project/archive' already exists.\n",
      "Successfully moved /data/Spark_project/group1/sales_agents_SS_raw_1.csv to Archive.\n",
      "Successfully copied /data/Spark_project/group1/sales_transactions_SS_raw_1.csv to /Spark_Project/data/Q_company/sales_transactions/group1_sales_transactions_SS_raw_1.csv in HDFS.\n",
      "Folder '/data/Spark_project/archive' already exists.\n",
      "Successfully moved /data/Spark_project/group1/sales_transactions_SS_raw_1.csv to Archive.\n"
     ]
    }
   ],
   "source": [
    "move_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9e4a6b45-bb5d-4a21-828a-5bbcc42171b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/Spark_project/archive/group1/branches_SS_raw_1.csv',\n",
       " '/data/Spark_project/archive/group1/sales_agents_SS_raw_1.csv',\n",
       " '/data/Spark_project/archive/group1/sales_transactions_SS_raw_1.csv']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d27c5d-d6fc-4fde-913f-099effb5a95b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
